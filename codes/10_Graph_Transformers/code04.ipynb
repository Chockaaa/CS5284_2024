{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 04 : Graph Transformers with edge features and PyTorch (dense linear algebra)\n",
    "\n",
    "### Xavier Bresson\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit import RDLogger   \n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset/QM9_1.4k_pytorch/\n",
      "Time: 0.2808411121368408\n",
      "num train data : 1000\n",
      "atom_dict.idx2word : ['N', 'C', 'O', 'F', 'N H3 +', 'O -', 'C H1 -', 'N +', 'N -']\n",
      "atom_dict.word2idx : {'N': 0, 'C': 1, 'O': 2, 'F': 3, 'N H3 +': 4, 'O -': 5, 'C H1 -': 6, 'N +': 7, 'N -': 8}\n",
      "bond_dict.idx2word : ['NONE', 'SINGLE', 'DOUBLE', 'TRIPLE']\n",
      "bond_dict.word2idx : {'NONE': 0, 'SINGLE': 1, 'DOUBLE': 2, 'TRIPLE': 3}\n",
      "9 4\n",
      "train[idx].atom_type : tensor([1, 1, 2, 1, 1, 1, 1])\n",
      "train[idx].atom_type_pe : tensor([0, 1, 0, 2, 3, 4, 5])\n",
      "train[idx].bond_type : tensor([[0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 0]])\n",
      "train[idx].bag_of_atoms : tensor([0, 6, 1, 0, 0, 0, 0, 0, 0])\n",
      "train[idx].smile:  CC(O)C1CC1C\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x31f6eecf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "start = time.time()\n",
    "\n",
    "data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'\n",
    "print(data_folder_pytorch)\n",
    "\n",
    "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
    "    atom_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
    "    bond_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"test_pytorch.pkl\",\"rb\") as f:\n",
    "    test=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"val_pytorch.pkl\",\"rb\") as f:\n",
    "    val=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"train_pytorch.pkl\",\"rb\") as f:\n",
    "    train=pickle.load(f)\n",
    "print('Time:',time.time()-start)\n",
    "\n",
    "print('num train data :',len(train)) \n",
    "\n",
    "print('atom_dict.idx2word :',atom_dict.idx2word)\n",
    "print('atom_dict.word2idx :',atom_dict.word2idx)\n",
    "print('bond_dict.idx2word :',bond_dict.idx2word)\n",
    "print('bond_dict.word2idx :',bond_dict.word2idx)\n",
    "\n",
    "num_atom_type = len(atom_dict.idx2word)\n",
    "num_bond_type = len(bond_dict.idx2word)\n",
    "print(num_atom_type, num_bond_type)\n",
    "\n",
    "idx = 45\n",
    "print('train[idx].atom_type :',train[idx].atom_type)\n",
    "print('train[idx].atom_type_pe :',train[idx].atom_type_pe)\n",
    "print('train[idx].bond_type :',train[idx].bond_type)\n",
    "print('train[idx].bag_of_atoms :',train[idx].bag_of_atoms)\n",
    "print('train[idx].smile: ',train[idx].smile)\n",
    "mol = Chem.MolFromSmiles(train[idx].smile)\n",
    "mol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "Max num atoms =  9\n",
      "Train\n",
      "number of molecule of size 4: \t 1\n",
      "number of molecule of size 5: \t 1\n",
      "number of molecule of size 6: \t 7\n",
      "number of molecule of size 7: \t 24\n",
      "number of molecule of size 8: \t 136\n",
      "number of molecule of size 9: \t 831\n",
      "Val\n",
      "number of molecule of size 7: \t 6\n",
      "number of molecule of size 8: \t 28\n",
      "number of molecule of size 9: \t 166\n",
      "Test\n",
      "number of molecule of size 6: \t 1\n",
      "number of molecule of size 7: \t 3\n",
      "number of molecule of size 8: \t 37\n",
      "number of molecule of size 9: \t 159\n"
     ]
    }
   ],
   "source": [
    "# Organize data into group of of molecules of fixed sized\n",
    "# Example: train[22] is a list containing all the molecules of size 22  \n",
    "def group_molecules_per_size(dataset):\n",
    "    mydict={}\n",
    "    for mol in dataset:\n",
    "        if len(mol) not in mydict:\n",
    "            mydict[len(mol)]=[]\n",
    "        mydict[len(mol)].append(mol)\n",
    "    return mydict\n",
    "test_group  = group_molecules_per_size(test)\n",
    "val_group   = group_molecules_per_size(val)\n",
    "train_group = group_molecules_per_size(train)\n",
    "print(len(train_group[8])) # QM9\n",
    "# print(len(train_group[28])) # ZINC\n",
    "\n",
    "# what is the biggest molecule in the train set\n",
    "max_mol_sz= max(list( train_group.keys()))\n",
    "print('Max num atoms = ', max_mol_sz)\n",
    "\n",
    "# print distribution w.r.t. molecule size\n",
    "def print_distribution(data):\n",
    "    for nb_atom in range(max_mol_sz+1):\n",
    "        try: \n",
    "            print('number of molecule of size {}: \\t {}'.format(nb_atom, len(data[nb_atom])))\n",
    "        except:\n",
    "            pass\n",
    "print('Train'); print_distribution(train_group)\n",
    "print('Val'); print_distribution(val_group)\n",
    "print('Test'); print_distribution(test_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate batch of pytorch molecules of same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "num_batches_remaining : {9: 17, 8: 3, 7: 1, 6: 1, 5: 1, 4: 1}\n",
      "sz : 9\n",
      "indices : 50 [468 415 334 223 495 570 462 421 679 406 365 698 414 798 446 188 690 543\n",
      " 763  59 180 706  75 393 488 473 309 177 146 700 497 655 135 216 728 732\n",
      "  71 305 205  68 625 774 239 343 500 735  67 186 795 563]\n",
      "minibatch_node : torch.Size([50, 9])\n",
      "minibatch_pe : torch.Size([50, 9])\n",
      "minibatch_edge : torch.Size([50, 9, 9])\n",
      "minibatch_boa : torch.Size([50, 9])\n"
     ]
    }
   ],
   "source": [
    "# A class to help drawing batches of molecules having the same size\n",
    "class MoleculeSampler:\n",
    "    def __init__(self, organized_dataset, bs , shuffle=True):  \n",
    "        self.bs = bs\n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.counter = { sz: 0   for sz in organized_dataset }\n",
    "        if shuffle:\n",
    "            self.order = { sz: np.random.permutation(num)  for sz , num in self.num_mol.items() }\n",
    "        else:\n",
    "            self.order = { sz: np.arange(num)  for sz , num in self.num_mol.items() } \n",
    "\n",
    "    def compute_num_batches_remaining(self):\n",
    "        #return {sz:  ( self.num_mol[sz] - self.counter[sz] ) // self.bs  for sz in self.num_mol} \n",
    "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol} \n",
    "\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   ) \n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "\n",
    "    def is_empty(self):\n",
    "        num_batches= self.compute_num_batches_remaining()\n",
    "        return sum( num_batches.values() ) == 0\n",
    "\n",
    "    def draw_batch_of_molecules(self, sz):  \n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        if (self.num_mol[sz] - self.counter[sz])/self.bs >= 1.0:\n",
    "            bs = self.bs\n",
    "        else:\n",
    "            bs = self.num_mol[sz] - (self.num_mol[sz]//self.bs) * self.bs\n",
    "        #print('sz, bs',sz, bs)\n",
    "        indices = self.order[sz][ self.counter[sz] : self.counter[sz] + bs]\n",
    "        self.counter[sz] += bs \n",
    "        return indices\n",
    "\n",
    "# extract one mini-batch\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "while(not sampler.is_empty()):\n",
    "    num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "    print('num_batches_remaining :',num_batches_remaining)\n",
    "    sz = sampler.choose_molecule_size()\n",
    "    print('sz :',sz)\n",
    "    indices = sampler.draw_batch_of_molecules(sz) \n",
    "    print('indices :',len(indices),indices)\n",
    "    minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] )\n",
    "    print('minibatch_node :',minibatch_node.size())\n",
    "    minibatch_pe  = torch.stack( [ train_group[sz][i].atom_type_pe  for i in indices] )\n",
    "    print('minibatch_pe :',minibatch_pe.size())\n",
    "    minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] )\n",
    "    print('minibatch_edge :',minibatch_edge.size())\n",
    "    minibatch_boa = torch.stack( [ train_group[sz][i].bag_of_atoms for i in indices] )\n",
    "    print('minibatch_boa :',minibatch_boa.size())\n",
    "    break\n",
    "    print('---------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute valid molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_size.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "sz : 9\n"
     ]
    }
   ],
   "source": [
    "# A class to sample a molecule size w.r.t. the train distribution\n",
    "class sample_molecule_size:\n",
    "    def __init__(self, organized_dataset):  \n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.num_batches_remaining = { sz:  self.num_mol[sz]  for sz in self.num_mol } \n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.num_batches_remaining\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   ) \n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "        \n",
    "sampler_size = sample_molecule_size(train_group)\n",
    "print('sampler_size.num_mol :',sampler_size.num_mol)\n",
    "sz = sampler_size.choose_molecule_size()\n",
    "print('sz :',sz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the class of dense GraphTransformer networks with edge features \n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=&  h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell}))  \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in V} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in V} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=&  e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell}))  \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{N\\times N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{N\\times N\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Differences between sparse GT and dense GT:\n",
    "- The edge features are now dense : Sparse $e\\in \\mathbb{R}^{E\\times d}$ to $e\\in \\mathbb{R}^{N\\times N\\times d}$.\n",
    "- The absence of edges in the graph is now considered as a bond type e.g. `None` type with integer value `0`.\n",
    "- The attention function is now connected to all nodes in the graph : Sparse attention with $\\sum_{j\\in \\mathcal{N}_i}$ to dense attention $\\sum_{j\\in V}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, num_heads, num_layers, drop :  128 8 4 0.0\n",
      "num_warmup : 40\n"
     ]
    }
   ],
   "source": [
    "# Global constants\n",
    "num_heads = 8; d = 16*num_heads; num_layers = 4; drop = 0.0; bs = 50 \n",
    "print('d, num_heads, num_layers, drop : ', d, num_heads, num_layers, drop)  \n",
    "\n",
    "# Warmup \n",
    "num_mol_size = 20\n",
    "num_warmup = 2 * max( num_mol_size, len(train) // bs ) # 4 epochs * max( num_mol_size=20, num_mol/batch_size)\n",
    "print('num_warmup :',num_warmup)\n",
    "\n",
    "# num_batch_one_epoch = len(train) // bs\n",
    "# # select the number of epochs to get the horizon of influence, e.g. mu^N = 0.001\n",
    "# wanted_num_epochs = 50\n",
    "# wanted_num_batches = wanted_num_epochs * num_batch_one_epoch # = N\n",
    "# # log( mu^N ) = log(0.001) => N log(mu) = log(0.001) => mu = exp( log(0.001)/N )\n",
    "# mu_ema = torch.exp( torch.log(torch.tensor(0.001)) / wanted_num_batches).item()\n",
    "# if mu_ema>=0.9999: mu_ema = 0.9999\n",
    "# # print('wanted_num_epochs, num_batch_one_epoch, wanted_num_batches, mu_ema : ', wanted_num_epochs, num_batch_one_epoch, wanted_num_batches, mu_ema)\n",
    "# # checking the value mu_ema\n",
    "# num_batch_mu_ema_zero = torch.log(torch.tensor(0.001)) / torch.log(torch.tensor(mu_ema))\n",
    "# # print('mu_ema, num_batch_mu_ema_zero, mu_ema**num_batch_mu_ema_zero(0.001) : ', mu_ema, int(num_batch_mu_ema_zero.item()), mu_ema**num_batch_mu_ema_zero.item())\n",
    "# print('num_epochs_horizon_ema : ', num_batch_mu_ema_zero.item() / num_batch_one_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firt run\n",
      "Number of parameters: 1588225 (1.59 million)\n",
      "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "num_batches_remaining : {9: 17, 8: 3, 7: 1, 6: 1, 5: 1, 4: 1}\n",
      "sz : 4\n",
      "indices : 1 [0]\n",
      "minibatch_node : torch.Size([1, 4])\n",
      "minibatch_edge : torch.Size([1, 4, 4])\n",
      "batch_target : torch.Size([1, 1])\n",
      "batch_x_pred torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define DDPM architecture\n",
    "class head_attention(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x) # [bs, n, d_head]\n",
    "        K = self.K(x) # [bs, n, d_head]\n",
    "        V = self.V(x) # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        E = self.E(e) # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(x).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(x).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = Ni + Nj + E\n",
    "        Att = (Q * e * K).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1) # [bs, n, n]\n",
    "        Att = self.drop_att(Att)      \n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e\n",
    "        \n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d, num_heads):  \n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        self.heads = nn.ModuleList( [head_attention(d, d_head) for _ in range(num_heads)] )\n",
    "        self.WOx = nn.Linear(d, d)\n",
    "        self.WOe = nn.Linear(d, d)\n",
    "        self.drop_x = nn.Dropout(drop)\n",
    "        self.drop_e = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x_MHA = []\n",
    "        e_MHA = []    \n",
    "        for head in self.heads:\n",
    "            x_HA, e_HA = head(x,e) # [bs, n, d_head], [bs, n, n, d_head]\n",
    "            x_MHA.append(x_HA)\n",
    "            e_MHA.append(e_HA)\n",
    "        x = self.WOx(torch.cat(x_MHA, dim=2)) # [bs, n, d]\n",
    "        x = self.drop_x(x)\n",
    "        e = self.WOe(torch.cat(e_MHA, dim=3)) # [bs, n, n, d]\n",
    "        e = self.drop_e(e)\n",
    "        return x, e\n",
    "\n",
    "class BlockGT(nn.Module):\n",
    "    def __init__(self, d, num_heads):  \n",
    "        super().__init__()\n",
    "        self.LNx = nn.LayerNorm(d)\n",
    "        self.LNe = nn.LayerNorm(d)\n",
    "        self.LNx2 = nn.LayerNorm(d)\n",
    "        self.LNe2 = nn.LayerNorm(d)\n",
    "        self.MHA = MHA(d, num_heads)\n",
    "        self.MLPx = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MLPe = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "        self.drop_e_mlp = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x = self.LNx(x)\n",
    "        e = self.LNe(e)\n",
    "        x_MHA, e_MHA = self.MHA(x, e) # [bs, n, d], [bs, n, n, d]\n",
    "        x = x + x_MHA # [bs, n, d]\n",
    "        x = x + self.MLPx(self.LNx2(x)) # [bs, n, d]\n",
    "        x = self.drop_x_mlp(x)\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLPe(self.LNe2(e)) # [bs, n, n, d]\n",
    "        e = self.drop_e_mlp(e)\n",
    "        return x, e\n",
    "       \n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0,3,1,2)\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2)\n",
    "    mask = (triu.abs()>0).float()\n",
    "    x =  x * (1 - mask ) + mask * triu\n",
    "    x = x.permute(0,2,3,1)\n",
    "    return x\n",
    "    \n",
    "class GT(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_atom_type, d)\n",
    "        self.bond_emb = nn.Embedding(num_bond_type, d)\n",
    "        num_layers_encoder = 4\n",
    "        self.BlockGT_encoder_layers = nn.ModuleList( [BlockGT(d, num_heads) for _ in range(num_layers_encoder)] )\n",
    "        self.ln_x_final = nn.LayerNorm(d)  \n",
    "        self.linear_x_final = nn.Linear(d, 1, bias=True)\n",
    "        self.drop_x_emb = nn.Dropout(drop)\n",
    "        self.drop_e_emb = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "\n",
    "        # input layer\n",
    "        x = self.atom_emb(x)                   # [bs, n, d]\n",
    "        e = self.bond_emb(e)                   # [bs, n, n, d]\n",
    "        e = sym_tensor(e)                      # [bs, n, n, d]\n",
    "        x = self.drop_x_emb(x)\n",
    "        e = self.drop_e_emb(e)\n",
    "        \n",
    "        # encoder\n",
    "        for gt_layer in self.BlockGT_encoder_layers:\n",
    "            x, e = gt_layer(x, e)  # [bs, n, d],  [bs, n, n, d]\n",
    "            e = sym_tensor(e)\n",
    "\n",
    "        # class token\n",
    "        mol_token = x.mean(1) # [bs, d]\n",
    "\n",
    "        # regressor\n",
    "        x = self.ln_x_final(mol_token)\n",
    "        x = self.linear_x_final(x)\n",
    "\n",
    "        return x   \n",
    "\n",
    "try:\n",
    "    del net\n",
    "except:\n",
    "  print(\"Firt run\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Instantiate the network\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :',num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :',sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz) \n",
    "print('indices :',len(indices),indices)\n",
    "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "print('minibatch_node :',minibatch_node.size())\n",
    "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "print('minibatch_edge :',minibatch_edge.size())\n",
    "batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "print('batch_target :',batch_target.size())\n",
    "\n",
    "batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "print('batch_x_pred',batch_x_pred.size())\n",
    "\n",
    "loss = nn.L1Loss()(batch_x_pred, batch_target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1588225 (1.59 million)\n",
      "num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current): 40 2 0\n",
      "epoch= 0\t time= 0.1448 min\t lr= 0.0000625\t train_loss= 1.3886\t test_loss= 1.3653\n",
      "epoch= 1\t time= 0.5361 min\t lr= 0.0001000\t train_loss= 1.3860\t test_loss= 1.2622\n",
      "epoch= 2\t time= 0.6973 min\t lr= 0.0001000\t train_loss= 1.2971\t test_loss= 1.3485\n",
      "epoch= 3\t time= 1.0201 min\t lr= 0.0001000\t train_loss= 1.3401\t test_loss= 1.3251\n",
      "epoch= 4\t time= 1.1846 min\t lr= 0.0001000\t train_loss= 1.2790\t test_loss= 1.3068\n",
      "epoch= 5\t time= 1.4658 min\t lr= 0.0001000\t train_loss= 1.2723\t test_loss= 1.2913\n",
      "epoch= 6\t time= 1.6312 min\t lr= 0.0001000\t train_loss= 1.2817\t test_loss= 1.2801\n",
      "Epoch 00007: reducing learning rate of group 0 to 9.5000e-05.\n",
      "epoch= 7\t time= 1.8142 min\t lr= 0.0000950\t train_loss= 1.3101\t test_loss= 1.2341\n",
      "epoch= 8\t time= 2.0682 min\t lr= 0.0000950\t train_loss= 1.2725\t test_loss= 1.2230\n",
      "epoch= 9\t time= 2.2490 min\t lr= 0.0000950\t train_loss= 1.2588\t test_loss= 1.2423\n",
      "epoch= 10\t time= 2.4750 min\t lr= 0.0000950\t train_loss= 1.2542\t test_loss= 1.2133\n",
      "epoch= 11\t time= 2.6397 min\t lr= 0.0000950\t train_loss= 1.2519\t test_loss= 1.2064\n",
      "epoch= 12\t time= 2.9345 min\t lr= 0.0000950\t train_loss= 1.1972\t test_loss= 1.2088\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "del net\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0003\n",
    "init_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
    "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
    "\n",
    "num_warmup_batch = 0\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 100\n",
    "\n",
    "lossMAE = nn.L1Loss()\n",
    "\n",
    "print('num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current):', \\\n",
    "      num_warmup, num_warmup//(len(train)//bs), num_warmup_batch)\n",
    "\n",
    "total_loss = moving_loss = -1\n",
    "list_loss = []\n",
    "start=time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_data = 0\n",
    "    net.train()\n",
    "\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(train_group, bs)\n",
    "    #print('sampler.num_mol :',sampler.num_mol)\n",
    "    while(not sampler.is_empty()):\n",
    "        num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "        #print('num_batches_remaining :',num_batches_remaining)\n",
    "        sz = sampler.choose_molecule_size()\n",
    "        #print('sz :',sz)\n",
    "        indices = sampler.draw_batch_of_molecules(sz) \n",
    "        bs2 = len(indices)\n",
    "        #print('indices :',len(indices),indices)\n",
    "        batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "        #print('minibatch_node :',minibatch_node.size())\n",
    "        batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "        #print('minibatch_edge :',minibatch_edge.size())\n",
    "        batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "\n",
    "        batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "        \n",
    "        loss = lossMAE(batch_x_pred, batch_target)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if num_warmup_batch < num_warmup:\n",
    "            scheduler_warmup.step() # warmup scheduler\n",
    "        num_warmup_batch += 1\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += bs2 * loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_data += bs2\n",
    "\n",
    "\n",
    "    # TEST SET\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(test_group, bs)\n",
    "    running_test_loss = 0\n",
    "    num_test_data = 0\n",
    "    with torch.no_grad(): \n",
    "        while(not sampler.is_empty()):\n",
    "            num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "            sz = sampler.choose_molecule_size()\n",
    "            indices = sampler.draw_batch_of_molecules(sz) \n",
    "            bs2 = len(indices)\n",
    "            batch_x0 = minibatch_node = torch.stack( [ test_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "            batch_e0 = minibatch_edge = torch.stack( [ test_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "            batch_target = torch.stack( [ test_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "            batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "            running_test_loss += bs2 * lossMAE(batch_x_pred, batch_target).detach().item()\n",
    "            num_test_data += bs2\n",
    "            \n",
    "    \n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    mean_train_loss = running_loss/num_data\n",
    "    mean_test_loss = running_test_loss/num_test_data\n",
    "    if num_warmup_batch >= num_warmup:\n",
    "        scheduler_tracker.step(mean_train_loss) # tracker scheduler defined w.r.t. loss value\n",
    "        num_warmup_batch += 1\n",
    "    elapsed = (time.time()-start)/60\n",
    "    if not epoch%1:\n",
    "        line = 'epoch= ' + str(epoch) + '\\t time= ' + str(elapsed)[:6] + ' min' + '\\t lr= ' + \\\n",
    "        '{:.7f}'.format(optimizer.param_groups[0]['lr']) + '\\t train_loss= ' + str(mean_train_loss)[:6] + \\\n",
    "        '\\t test_loss= ' + str(mean_test_loss)[:6]\n",
    "        print(line)\n",
    "\n",
    "    # Check lr value \n",
    "    if optimizer.param_groups[0]['lr'] < 10**-6: # 2*10**-4: quick, # 10**-6: slow\n",
    "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "      break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| Sparse GT (DGL)   | 0.4483    | 0.7327    |\n",
    "| Dense GT (PyTroch)    | 0.5464   | 0.6954    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
