{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph SVM\n",
    "\n",
    "## Lab 01 : Standard/Linear SVM -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/04_Graph_SVM'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook \n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_purity\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_linearSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['Cgt_train'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Implement the linear SVM on linear separable data using the primal-dual iterative algorithm**\n",
    "\n",
    "*Hint:* Following Page 18-20, Lecture 4 Slides\n",
    "\n",
    "**Step 1:** Compute the Linear Kernel $Ker$ and $L, Q$ defined as\n",
    "- $Ker= XX^\\top$,\n",
    "- $L = \\text{diag}(l)$, \n",
    "- $Q = LKL$.\n",
    " \n",
    "You may use function `np.diag()`, the transpose operator `.T`, and the matrix-matrix multiplication operator `.dot()`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linear kernel, L, Q\n",
    "\n",
    "l = l_train\n",
    "\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "Ker = \n",
    "L = \n",
    "Q = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Initialize $\\alpha^{k=0} = 0_n$ and $\\beta^{k=0} = 1_n$.\n",
    "\n",
    "You may use function `np.zeros()` for initializing a zero vector, and function `np.ones()` for initializing a zero vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "alpha = \n",
    "beta = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Choose the time steps $\\tau_\\alpha, \\tau_\\beta$ such that $\\tau_\\alpha\\tau_\\beta \\leq \\frac{1}{\\|Q\\| \\cdot \\|L\\|}$.\n",
    "\n",
    "Some feasible choices can be $\\tau_\\alpha = \\frac{a}{\\|Q\\|}, \\tau_\\beta = \\frac{b}{\\|L\\|}$, where $ab \\leq 1$.\n",
    " \n",
    "For example: $\\tau_\\alpha = \\frac{1}{\\|Q\\|}, \\tau_\\beta = \\frac{1}{\\|L\\|}$.\n",
    "\n",
    "You may use `np.linalg.norm()` to compute the norm of a matrix.\n",
    "\n",
    "Try to evaluate the performance of linear SVM with different choices of time steps.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time steps\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "tau_alpha = \n",
    "tau_beta = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Project alpha to $[0, +\\infty]$ during the update of alpha and beta with conjuguate gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data and train data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "LKXtrain = L.dot(Xtrain.dot(Xtrain.T)) \n",
    "\n",
    "# Initialization\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) and (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    #print('k',k,num_iter,diff_alpha)\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha* Q.dot(alpha) - tau_alpha* l* beta \n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    \n",
    "    # Projection of alpha on [0,+infty]\n",
    "    ############################################################################\n",
    "    # Your code start\n",
    "    ############################################################################\n",
    "\n",
    "    alpha\n",
    "\n",
    "    ############################################################################\n",
    "    # Your code here\n",
    "    ############################################################################\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%5) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Approximate bias value \n",
    "        b = np.mean( l - alpha.T.dot(LKXtrain) )\n",
    "        \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_twomoons_softSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['C_train_errors'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "Cgt_train[:250] = 0; Cgt_train[250:] = 1\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: Compute linear kernel, L, Q, time steps, initialization and projection of alpha as for Question 1**\n",
    "\n",
    "- Compare the results with the linearly separable case and determine which performs better. \n",
    "\n",
    "- What strategy can be used to enhance the performance of SVM on non-linearly separable data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = # YOUR CODE HERE \n",
    "l = # YOUR CODE HERE \n",
    "L = # YOUR CODE HERE \n",
    "Q = # YOUR CODE HERE \n",
    "\n",
    "# Time steps\n",
    "tau_alpha = # YOUR CODE HERE \n",
    "tau_beta = # YOUR CODE HERE \n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "LKXtrain = L.dot(Xtrain.dot(Xtrain.T)) \n",
    "\n",
    "# Initialization\n",
    "alpha = # YOUR CODE HERE \n",
    "beta = # YOUR CODE HERE \n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) and (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    #print('k',k,num_iter,diff_alpha)\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha* Q.dot(alpha) - tau_alpha* l* beta \n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha# YOUR CODE HERE  # Projection on [0,+infty]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%5) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Approximate bias value #\n",
    "        b = np.mean( l - alpha.T.dot(LKXtrain) )\n",
    "        \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        size_vertex_plot = 33\n",
    "        plt.figure(figsize=(12,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        #plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
