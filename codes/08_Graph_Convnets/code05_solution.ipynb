{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Convolutional Networks\n",
    "\n",
    "## Lab 05 : GatedGCNs for chemical regression - Solution\n",
    "\n",
    "Bresson, Laurent, Residual Gated Graph ConvNets, 2017  \n",
    "https://arxiv.org/pdf/1711.07553\n",
    "\n",
    "Bresson, Laurent, A two-step graph convolutional decoder for molecule generation, 2019  \n",
    "https://arxiv.org/pdf/1906.03412\n",
    "\n",
    "\n",
    "### Xavier Bresson\n",
    "\n",
    "Notebook goals :\n",
    "- Implement a two-layer MLP for regression \n",
    "- Define the Mean Absolute Error (MAE) as regression loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/08_Graph_Convnets'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "from lib.utils import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "13\n",
      "4\n",
      "Loading datasets QM9_dgl...\n",
      "train, test, val sizes : 2000 200 200\n",
      "Time: 1.0331s\n",
      "2000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2623]), tensor([1.0908])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5063]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4348]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "\n",
    "print('Loading data')\n",
    "data_folder_pytorch = 'datasets/QM9_pytorch/'\n",
    "with open(data_folder_pytorch+\"train_pytorch.pkl\",\"rb\") as f:\n",
    "    dataset=pickle.load(f)\n",
    "\n",
    "# Load the number of atom and bond types \n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "data_folder_dgl = 'datasets/QM9_dgl/'\n",
    "dataset_name = 'QM9'\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a batch of graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=89, num_edges=192,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 0.6655],\n",
      "        [ 0.2697],\n",
      "        [-0.0046],\n",
      "        [-0.1494],\n",
      "        [ 1.7725],\n",
      "        [ 3.0502],\n",
      "        [-3.9406],\n",
      "        [-0.7122],\n",
      "        [-0.1688],\n",
      "        [-1.1287]])\n",
      "batch_norm_n: torch.Size([89, 1])\n",
      "batch_norm_e: torch.Size([192, 1])\n",
      "batch_x: torch.Size([89])\n",
      "batch_e: torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features\n",
    "def collate(samples):\n",
    "    \n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    \n",
    "    # Normalization w.r.t. graph sizes\n",
    "    tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n",
    "    tab_norm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n",
    "    batch_norm_n = torch.cat(tab_norm_n).sqrt()  \n",
    "    tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n",
    "    tab_norm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n",
    "    batch_norm_e = torch.cat(tab_norm_e).sqrt()\n",
    "    \n",
    "    return batch_graphs, batch_labels, batch_norm_n, batch_norm_e\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels, batch_norm_n, batch_norm_e = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "print('batch_norm_n:',batch_norm_n.size())\n",
    "print('batch_norm_e:',batch_norm_e.size())\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the class of GatedGCN networks with DGL\n",
    "\n",
    "Node and edge update equations:  \n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& h_i^{\\ell} + \\text{ReLU} \\Big( \\text{BN} \\Big( A^\\ell h_i^{\\ell} +  \\sum_{j\\sim i} \\eta(e_{ij}^{\\ell}) \\odot B^\\ell h_j^{\\ell} \\Big) \\Big), \\quad \\eta(e_{ij}^{\\ell}) = \\frac{\\sigma(e_{ij}^{\\ell})}{\\sum_{j'\\sim i} \\sigma(e_{ij'}^{\\ell}) + \\varepsilon} \\\\\n",
    "e_{ij}^{\\ell+1} &=& e^\\ell_{ij} + \\text{ReLU} \\Big( \\text{BN}  \\Big( C^\\ell e_{ij}^{\\ell} + D^\\ell h^{\\ell}_i + E^\\ell h^{\\ell}_j  \\Big) \\Big)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatedGCN_net(\n",
      "  (embedding_h): Embedding(13, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (GatedGCN_layers): ModuleList(\n",
      "    (0-3): 4 x GatedGCN_layer(\n",
      "      (A): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (B): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (C): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (D): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (E): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (bn_node_h): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_node_e): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (MLP_layer): MLP_layer(\n",
      "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a two-layer MLP for regression \n",
    "class MLP_layer(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim): \n",
    "        super(MLP_layer, self).__init__()\n",
    "        # You may use \"nn.Linear\"\n",
    "        #self.linear1 = ### YOUR CODE HERE\n",
    "        #self.linear2 = ### YOUR CODE HERE\n",
    "        self.linear1 = nn.Linear( input_dim, hidden_dim, bias=True )\n",
    "        self.linear2 = nn.Linear( hidden_dim, 1, bias=True )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #y = ### YOUR CODE HERE\n",
    "        y = self.linear2(torch.relu(self.linear1(x)))\n",
    "        return y\n",
    "\n",
    "\n",
    "# class of GatedGCN layer  \n",
    "class GatedGCN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GatedGCN_layer, self).__init__()\n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges):\n",
    "        Bhj = edges.src['Bh'] # Bhj with j/src\n",
    "        eij = edges.data['Ce'] +  edges.dst['Dh'] + edges.src['Eh'] # Ceij + Dhi + Ehj with dst/i, src/j\n",
    "        edges.data['e'] = eij # update edge feature value\n",
    "        return {'Bhj' : Bhj, 'eij' : eij} # send message={Bhj, eij} to node dst/i\n",
    "\n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={Bhj, eij} sent to node dst/i with Step 1\n",
    "    def reduce_func(self, nodes):\n",
    "        Ahi = nodes.data['Ah']\n",
    "        Bhj = nodes.mailbox['Bhj']\n",
    "        e = nodes.mailbox['eij'] \n",
    "        sigmaij = torch.sigmoid(e) # sigma_ij = sigmoid(e_ij)\n",
    "        h = Ahi + torch.sum( sigmaij * Bhj, dim=1 ) / torch.sum( sigmaij, dim=1 ) # hi = Ahi + sum_j eta_ij * Bhj    \n",
    "        return {'h' : h} # return update node feature hi\n",
    "    \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        h_in = h # residual connection\n",
    "        e_in = e # residual connection\n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) # linear transformation \n",
    "        g.ndata['Bh'] = self.B(h) # linear transformation \n",
    "        g.ndata['Dh'] = self.D(h) # linear transformation \n",
    "        g.ndata['Eh'] = self.E(h) # linear transformation \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) # linear transformation \n",
    "        \n",
    "        g.update_all(self.message_func,self.reduce_func) # update the node and edge features with DGL\n",
    "        \n",
    "        h = g.ndata['h'] # collect the node output of graph convolution\n",
    "        e = g.edata['e'] # collect the edge output of graph convolution\n",
    "        \n",
    "        h = h* snorm_n # normalize activation w.r.t. graph node size\n",
    "        e = e* snorm_e # normalize activation w.r.t. graph edge size\n",
    "        \n",
    "        h = self.bn_node_h(h) # batch normalization  \n",
    "        e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        h = torch.relu(h) # non-linear activation\n",
    "        e = torch.relu(e) # non-linear activation\n",
    "        \n",
    "        h = h_in + h # residual connection\n",
    "        e = e_in + e # residual connection\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    \n",
    "class GatedGCN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GatedGCN_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        self.GatedGCN_layers = nn.ModuleList([ GatedGCN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        e = self.embedding_e(e)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GGCN_layer in self.GatedGCN_layers:\n",
    "            h,e = GGCN_layer(g,h,e,snorm_n,snorm_e)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "    \n",
    "    def loss(self, y_scores, y_labels):\n",
    "        # Define the Mean Absolute Error (MAE) as regression loss \n",
    "        #loss = ### YOUR CODE HERE\n",
    "        loss = nn.L1Loss()(y_scores, y_labels)\n",
    "        return loss        \n",
    "    \n",
    "    def update(self, lr):       \n",
    "        update = torch.optim.Adam( self.parameters(), lr=lr )\n",
    "        return update\n",
    "\n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['L'] = 4\n",
    "net = GatedGCN_net(net_parameters)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbresson/miniconda3/envs/gnn_course/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, time 2.1291, train_loss: 1.3120, test_loss: 1.3154, val_loss: 1.2048\n",
      "Epoch 1, time 2.2207, train_loss: 1.2040, test_loss: 1.2078, val_loss: 1.0875\n",
      "Epoch 2, time 2.3090, train_loss: 1.1284, test_loss: 1.0789, val_loss: 0.9942\n",
      "Epoch 3, time 2.3936, train_loss: 1.0691, test_loss: 1.0179, val_loss: 0.9465\n",
      "Epoch 4, time 3.3875, train_loss: 1.0240, test_loss: 0.9682, val_loss: 0.8984\n",
      "Epoch 5, time 10.0094, train_loss: 0.9808, test_loss: 0.9257, val_loss: 0.8401\n",
      "Epoch 6, time 3.2492, train_loss: 0.9325, test_loss: 0.8988, val_loss: 0.8331\n",
      "Epoch 7, time 1.9639, train_loss: 0.8912, test_loss: 0.9024, val_loss: 0.8381\n",
      "Epoch 8, time 2.3410, train_loss: 0.8707, test_loss: 0.8160, val_loss: 0.7388\n",
      "Epoch 9, time 9.0112, train_loss: 0.8354, test_loss: 0.7647, val_loss: 0.6935\n",
      "Epoch 10, time 6.2583, train_loss: 0.8127, test_loss: 0.8578, val_loss: 0.8197\n",
      "Epoch 11, time 2.2236, train_loss: 0.8137, test_loss: 0.8440, val_loss: 0.7317\n",
      "Epoch 12, time 1.9146, train_loss: 0.7902, test_loss: 0.9719, val_loss: 0.9773\n",
      "Epoch 13, time 1.8124, train_loss: 0.7705, test_loss: 0.7040, val_loss: 0.6201\n",
      "Epoch 14, time 2.0686, train_loss: 0.7701, test_loss: 0.7218, val_loss: 0.6223\n",
      "Epoch 15, time 3.2800, train_loss: 0.7450, test_loss: 1.0326, val_loss: 1.0518\n",
      "Epoch 16, time 2.7147, train_loss: 0.7483, test_loss: 0.6912, val_loss: 0.6085\n",
      "Epoch 17, time 2.6520, train_loss: 0.7301, test_loss: 0.6743, val_loss: 0.5814\n",
      "Epoch 18, time 2.3801, train_loss: 0.7202, test_loss: 0.6807, val_loss: 0.6042\n",
      "Epoch 19, time 2.3928, train_loss: 0.7204, test_loss: 0.6502, val_loss: 0.5618\n",
      "Epoch 20, time 3.0712, train_loss: 0.7042, test_loss: 0.7082, val_loss: 0.6472\n",
      "Epoch 21, time 3.7284, train_loss: 0.6913, test_loss: 0.8871, val_loss: 0.7291\n",
      "Epoch 22, time 2.5008, train_loss: 0.6994, test_loss: 0.7451, val_loss: 0.7102\n",
      "Epoch 23, time 2.1481, train_loss: 0.6746, test_loss: 0.6420, val_loss: 0.5721\n",
      "Epoch 24, time 10.1085, train_loss: 0.6822, test_loss: 0.8135, val_loss: 0.7927\n",
      "Epoch 25, time 7.1201, train_loss: 0.6717, test_loss: 0.6716, val_loss: 0.5687\n",
      "Epoch 26, time 15.2084, train_loss: 0.6646, test_loss: 0.6786, val_loss: 0.6050\n",
      "Epoch 27, time 2.3349, train_loss: 0.6645, test_loss: 0.6868, val_loss: 0.5719\n",
      "Epoch 28, time 2.1900, train_loss: 0.6580, test_loss: 0.6530, val_loss: 0.5437\n",
      "Epoch 29, time 9.0994, train_loss: 0.6521, test_loss: 0.6423, val_loss: 0.5282\n",
      "Epoch 30, time 10.3386, train_loss: 0.6415, test_loss: 0.7146, val_loss: 0.6849\n",
      "Epoch 31, time 2.1331, train_loss: 0.6499, test_loss: 0.6316, val_loss: 0.5230\n",
      "Epoch 32, time 2.8810, train_loss: 0.6446, test_loss: 0.6298, val_loss: 0.5219\n",
      "Epoch 33, time 2.2028, train_loss: 0.6493, test_loss: 0.7467, val_loss: 0.6046\n",
      "Epoch 34, time 2.4848, train_loss: 0.6385, test_loss: 0.6577, val_loss: 0.5337\n",
      "Epoch 35, time 2.2703, train_loss: 0.6353, test_loss: 0.8992, val_loss: 0.9078\n",
      "Epoch 36, time 3.2777, train_loss: 0.6451, test_loss: 0.6380, val_loss: 0.5219\n",
      "Epoch 37, time 2.5875, train_loss: 0.6340, test_loss: 0.6443, val_loss: 0.5687\n",
      "Epoch 38, time 9.0285, train_loss: 0.6340, test_loss: 0.7687, val_loss: 0.7732\n",
      "Epoch 39, time 26.3882, train_loss: 0.6145, test_loss: 0.9119, val_loss: 0.7335\n",
      "Epoch 40, time 8.7305, train_loss: 0.6371, test_loss: 0.7156, val_loss: 0.5626\n",
      "Epoch 41, time 2.2807, train_loss: 0.6055, test_loss: 0.7608, val_loss: 0.5983\n",
      "Epoch 42, time 2.2987, train_loss: 0.6147, test_loss: 0.6119, val_loss: 0.5033\n",
      "Epoch 43, time 2.0145, train_loss: 0.6168, test_loss: 0.6185, val_loss: 0.5188\n",
      "Epoch 44, time 2.2629, train_loss: 0.6079, test_loss: 0.6786, val_loss: 0.6183\n",
      "Epoch 45, time 2.1646, train_loss: 0.6284, test_loss: 0.5947, val_loss: 0.5270\n",
      "Epoch 46, time 2.4285, train_loss: 0.6016, test_loss: 0.6503, val_loss: 0.5350\n",
      "Epoch 47, time 3.3738, train_loss: 0.6096, test_loss: 0.5916, val_loss: 0.4963\n",
      "Epoch 48, time 2.4918, train_loss: 0.5992, test_loss: 0.6083, val_loss: 0.5370\n",
      "Epoch 49, time 2.6117, train_loss: 0.5997, test_loss: 0.7025, val_loss: 0.5602\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_snorm_n = batch_snorm_n\n",
    "        batch_snorm_e = batch_snorm_e\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n",
    "        loss = net.loss(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 100\n",
    "net_parameters['L'] = 4\n",
    "net = GatedGCN_net(net_parameters)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    epoch_train_loss = run_one_epoch(net, train_loader, True)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False)\n",
    "        epoch_val_loss = run_one_epoch(net, val_loader, False)  \n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}, val_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss, epoch_val_loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
